import re
def clean_company_name(name):
# Uppercase for consistency
name = name.upper()
# Remove common legal suffixes
suffixes = [
r'\bINC\b', r'\bCORP\b', r'\bCORPORATION\b',
r'\bGROUP\b', r'\bGROEP\b',
r'\bNV\b', r'\bAG\b',
r'\bLTD\b', r'\bLIMITED\b', r'\bCO\b',
r'\bCLASS A\b', r'\bCLASS B\b', r'\bPLC\b'
]
for s in suffixes:
name = re.sub(s, '', name)
# Replace & with space
name = name.replace('&', ' ')
# Remove extra whitespace
name = re.sub(r'\s+', ' ', name).strip()
return name
import pandas as pd
data_holdings = pd.read_table('xzw0_full_holdings.csv', delimiter=',')
def build_keyword(name):
cleaned = clean_company_name(name)
# If very short, expand it
if len(cleaned) <= 4:
return f'"{cleaned} company"'
# Quote multi-word company names
if ' ' in cleaned:
return f'"{cleaned}"'
return cleaned
from gdeltdoc import GdeltDoc, Filters
import pandas as pd
import time
gd = GdeltDoc()
results = []
ESG_THEMES = [  # http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT
"WB_1786_ENVIRONMENTAL_SUSTAINABILITY",
"WB_2089_ETHICS_AND_CODES_OF_CONDUCT",
"WB_417_CORPORATE_GOVERNANCE"
]
TRUSTED_DOMAINS = [
"reuters.com",
"bloomberg.com",
"ft.com",
"wsj.com",
"nytimes.com",
"theguardian.com",
"economist.com"
]
for _, row in data_holdings.iterrows():
keyword = build_keyword(row["Name"])
for theme in ESG_THEMES:
try:
f = Filters(
keyword=keyword,
language="English",
theme=theme,
#domain=TRUSTED_DOMAINS,
start_date="2023-01-01",
end_date="2025-12-26"
)
articles = gd.article_search(f)
if articles is None or len(articles) == 0:
continue
gkg = pd.DataFrame(articles)
# Add metadata
gkg["ISIN"] = row["ISIN"]
gkg["Name"] = row["Name"]
gkg["Theme"] = theme
gkg = gkg[gkg["title"].str.contains(keyword, na=False)]  # Title must contain company name
results.append(gkg)
time.sleep(0.5)  # avoid throttling
except Exception as e:
print(f"Error for {row['Name']} | Theme {theme}: {e}")
# Combine all results
news_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()
View(news_df)
View(data_holdings)
data_holdings[1,2]
data_holdings.["Name"][2]
data_holdings["Name"][2]
build_keyword(data_holdings["Name"][2])
build_keyword(data_holdings["Name"][6])
build_keyword(data_holdings["Name"][11])
build_keyword(data_holdings["Name"][14])
build_keyword(data_holdings["Name"][18])
build_keyword(data_holdings["Name"][25])
build_keyword(data_holdings["Name"][100])
View(news_df)
from gdeltdoc import GdeltDoc, Filters
import pandas as pd
import time
gd = GdeltDoc()
results = []
ESG_THEMES = [  # http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT
"WB_1786_ENVIRONMENTAL_SUSTAINABILITY",
"WB_2089_ETHICS_AND_CODES_OF_CONDUCT",
"WB_417_CORPORATE_GOVERNANCE",
"WB_2507_HUMAN_RIGHTS_ABUSES_AND_VIOLATIONS"
]
TRUSTED_DOMAINS = [
"reuters.com",
"bloomberg.com",
"ft.com",
"wsj.com",
"nytimes.com",
"theguardian.com",
"economist.com"
]
for _, row in data_holdings.iterrows():
keyword = build_keyword(row["Name"])
for theme in ESG_THEMES:
try:
f = Filters(
keyword=keyword,
language="English",
theme=theme,
#domain=TRUSTED_DOMAINS,
start_date="2023-01-01",
end_date="2025-12-26"
)
articles = gd.article_search(f)
if articles is None or len(articles) == 0:
continue
gkg = pd.DataFrame(articles)
# Add metadata
gkg["ISIN"] = row["ISIN"]
gkg["Name"] = row["Name"]
gkg["Theme"] = theme
gkg = gkg[gkg["title"].str.contains(keyword, na=False)]  # Title must contain company name
results.append(gkg)
time.sleep(0.5)  # avoid throttling
except Exception as e:
print(f"Error for {row['Name']} | Theme {theme}: {e}")
# Combine all results
news_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()
from gdeltdoc import GdeltDoc, Filters
import pandas as pd
import time
gd = GdeltDoc()
results = []
ESG_THEMES = [  # http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT
"WB_1786_ENVIRONMENTAL_SUSTAINABILITY",
"WB_2089_ETHICS_AND_CODES_OF_CONDUCT",
"WB_417_CORPORATE_GOVERNANCE",
"WB_2507_HUMAN_RIGHTS_ABUSES_AND_VIOLATIONS"
]
TRUSTED_DOMAINS = [
"reuters.com",
"bloomberg.com",
"ft.com",
"wsj.com",
"nytimes.com",
"theguardian.com",
"economist.com"
]
for _, row in data_holdings.iterrows():
keyword = build_keyword(row["Name"])
for theme in ESG_THEMES:
try:
f = Filters(
keyword=keyword,
language="English",
theme=theme,
#domain=TRUSTED_DOMAINS,
start_date="2023-01-01",
end_date="2025-12-26"
)
articles = gd.article_search(f)
if articles is None or len(articles) == 0:
continue
gkg = pd.DataFrame(articles)
# Add metadata
gkg["ISIN"] = row["ISIN"]
gkg["Name"] = row["Name"]
gkg["Theme"] = theme
results.append(gkg)
time.sleep(0.5)  # avoid throttling
except Exception as e:
print(f"Error for {row['Name']} | Theme {theme}: {e}")
# Combine all results
news_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()
from gdeltdoc import GdeltDoc, Filters
import pandas as pd
import time
gd = GdeltDoc()
results = []
ESG_THEMES = [  # http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT
"WB_1786_ENVIRONMENTAL_SUSTAINABILITY",
"WB_2089_ETHICS_AND_CODES_OF_CONDUCT",
"WB_417_CORPORATE_GOVERNANCE",
"WB_2507_HUMAN_RIGHTS_ABUSES_AND_VIOLATIONS"
]
TRUSTED_DOMAINS = [
"reuters.com",
"bloomberg.com",
"ft.com",
"wsj.com",
"nytimes.com",
"theguardian.com",
"economist.com"
]
for _, row in data_holdings.iterrows():
keyword = build_keyword(row["Name"])
for theme in ESG_THEMES:
try:
f = Filters(
keyword=keyword,
language="English",
theme=theme,
#domain=TRUSTED_DOMAINS,
start_date="2023-01-01",
end_date="2025-12-26"
)
articles = gd.article_search(f)
if articles is None or len(articles) == 0:
continue
gkg = pd.DataFrame(articles)
# Add metadata
gkg["ISIN"] = row["ISIN"]
gkg["Name"] = row["Name"]
gkg["Theme"] = theme
results.append(gkg)
time.sleep(0.5)  # avoid throttling
except Exception as e:
print(f"Error for {row['Name']} | Theme {theme}: {e}")
# Combine all results
news_df_original = pd.concat(results, ignore_index=True) if results else pd.DataFrame()
View(news_df_original)
# Delete rows with no title.
news_df = news_df_original.dropna(subset=["title"])
news_df = news_df[news_df["title"].str.strip() != ""]
# The (first part of the) company name must be in the title of the article.
import re
def name_in_title(row):
if pd.isna(row["title"]) or pd.isna(row["Name"]):
return False
# Use first token to avoid regex length issues
token = row["Name"].split()[0]
return re.search(rf"\b{re.escape(token)}\b", row["title"], re.IGNORECASE) is not None
news_df = news_df[news_df.apply(name_in_title, axis=1)]
# Maximally remove identical titles.
news_df = news_df.drop_duplicates(subset="title", keep="first")
View(news_df)
library(reticulate)
reticulate::use_virtualenv("esg-py", required = TRUE)
reticulate::repl_python()
library(data.table)
# Function to calculate percentage differences between consecutive years
calculate_percentage_differences <- function(dt, count_var, year_var, grouping_var) {
# Ensure data is ordered by year
dt <- dt[order(get(year_var))]
# If grouping_var is empty, don't group, just calculate the percentage difference for the entire dataset
if (length(grouping_var) > 0) {
dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = "lag")) / shift(.SD[[count_var]], type = "lag")) * 100, by = grouping_var]
} else {
dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = "lag")) / shift(.SD[[count_var]], type = "lag")) * 100]
}
return(dt)
}
# Helper function to calculate deviation
calculate_deviation <- function(actual, expected, metric) {
if (metric == "mad") return(mean(abs(actual - expected), na.rm = TRUE))
if (metric == "mse") return(mean((actual - expected)^2, na.rm = TRUE))
if (metric == "rmse") return(sqrt(mean((actual - expected)^2, na.rm = TRUE)))
if (is.function(metric)) return(metric(actual, expected)) # Allow user-defined metrics
stop("Unsupported deviation metric. Use 'mad', 'mse', 'rmse', or a custom function.")
}
forward_selection_trends <- function(dt, included_vars, optional_vars, count_var = "Count",
year_var = "Year", agg_func = sum, min_row_threshold = 10,
stopping_threshold = 0.01, deviation_metric = "mad") {
# Ensure the data is a data.table
dt <- as.data.table(dt)
included_vars <- as.character(included_vars); selected_vars <- included_vars
optional_vars <- as.character(optional_vars); remaining_vars <- optional_vars
# Check for missing data
if (anyNA(dt)) {
stop("The dataset contains missing values. Please clean or impute missing data before proceeding.")
}
selected_variables_and_deviations_at_each_step <- list() # To store selected variable and deviation at each step
iteration_details <- list() # To store details of all variables in each iteration
previous_deviation <- NA # Track deviation improvement
start_time <- Sys.time()
cat("Starting forward selection with minimum row threshold =", min_row_threshold, "...\n")
# Iterate until no remaining variable improves variability
for (step in seq_along(optional_vars)) {
# Check if there are any remaining vars left. If not, exit.
if (length(remaining_vars) == 0) break
# Calculate the reference percentage differences to be compared with
dt_ref_aggreg <- dt[, .(agg_count = agg_func(.SD[[count_var]])), by = c(year_var, selected_vars)]
ref_perc_diffs <- calculate_percentage_differences(copy(dt_ref_aggreg), count_var = 'agg_count', year_var, grouping_var = c(selected_vars))
# Track deviations for all remaining variables
results <- lapply(remaining_vars, function(var) {
# Aggregate the count variable using the custom function
grouped <- dt[, agg_count := agg_func(.SD[[count_var]]), by = c(selected_vars, var, year_var)]
# Ensure enough rows exist in each group
if (min(grouped[, .N, by = c(selected_vars, var)]$N) < min_row_threshold) {
return(NA) # Skip variables that would result in too few rows in groups
}
# Calculate percentage differences for each group
grouped_perc_diffs <- calculate_percentage_differences(copy(grouped), count_var = 'agg_count', year_var, grouping_var = c(selected_vars, var))
View(grouped_perc_diffs)
# Join with overall percentage differences to compare
comparison <- merge(grouped_perc_diffs, ref_perc_diffs,
by = c(selected_vars, year_var), all.x = TRUE)
View(comparison)
# Compute the deviation using the specified metric
calculate_deviation(comparison$perc_diff.x, comparison$perc_diff.y, deviation_metric)
})
# Skip step if all remaining variables fail the row count threshold
if (all(is.na(results))) {
cat("Stopping: No variable meets the minimum row threshold.\n")
break
}
# Capture deviations for all variables in the current iteration
iteration_deviation <- data.table(
Step = step,
Variable = remaining_vars,
Deviation = unlist(results)
)
iteration_details[[step]] <- iteration_deviation
# Find the variable with the maximum deviation
deviations_step <- unlist(results)
max_deviation <- max(deviations_step, na.rm = TRUE)
max_var <- remaining_vars[which.max(deviations_step)]
# Stop if the deviation improvement is below the threshold
if (!is.na(previous_deviation) && (max_deviation - previous_deviation) / abs(previous_deviation) < stopping_threshold) {
cat("Stopping: Deviation improvement below threshold.\n")
break
}
previous_deviation <- max_deviation
# Record the result
selected_variables_and_deviations_at_each_step[[step]] <- data.table(Step = step, Variable_with_max_deviation = max_var, Max_deviation = max_deviation)
# Add the selected variable to included and remove from remaining
selected_vars <- c(selected_vars, max_var)
remaining_vars <- setdiff(remaining_vars, max_var)
cat("Step", step, "- Variable selected:", max_var, "Deviation:", max_deviation, "\n")
}
end_time <- Sys.time()
# Final summary report
report <- list(
total_iterations = length(selected_variables_and_deviations_at_each_step),
selected_vars = selected_vars,
excluded_vars = setdiff(optional_vars, selected_vars),
execution_time = end_time - start_time,
deviation_metric = deviation_metric
)
return(list(
selected_variables_and_deviations_at_each_step = rbindlist(selected_variables_and_deviations_at_each_step), # Final selection of variables with their deviations
iteration_details = rbindlist(iteration_details), # Details of all iterations
report = report # Summary report
))
}
set.seed(42)
# Define dimensions
years <- 2000:2010
regions <- c("North", "South", "East", "West")
products <- c("A", "B")
channels <- c("Online", "Retail")
# Create base table with all combinations
dt <- CJ(
Year = years,
Region = regions,
Product = products,
Channel = channels
)
# Base count
dt[, base := 100]
# Region-specific trends (strong signal)
region_trend <- c(
North = 0.06,
South = 0.03,
East  = 0.00,
West  = -0.02
)
# Product-specific trends (moderate signal)
product_trend <- c(
A = 0.02,
B = 0.00
)
# Channel-specific noise (small)
channel_noise <- c(
Online = 0.005,
Retail = -0.005
)
# Generate counts: exponential growth + small noise
dt[, Count :=
base *
exp(
region_trend[Region] * (Year - min(Year)) +
product_trend[Product] * (Year - min(Year)) +
channel_noise[Channel] * (Year - min(Year))
) +
rnorm(.N, mean = 0, sd = 2)  # small random noise
]
# Ensure positive integer counts
dt[, Count := pmax(round(Count), 1)]
# Remove helper column
dt[, base := NULL]
# Check a few rows
head(dt)
# output res
res <- forward_selection_trends(
dt,
included_vars = character(0),
optional_vars = c("Region", "Product", "Channel"),
count_var = "Count",
year_var = "Year",
min_row_threshold = 10,
deviation_metric = "mad"
)
library(ggplot2)
# Step 1: Compute aggregated counts by Year and Region
dt_region <- dt[, .(agg_count = sum(Count)), by = .(Year, Region)]
dt_region <- calculate_percentage_differences(dt_region, count_var = "agg_count",
year_var = "Year", grouping_var = c("Region"))
# Step 3: Plot percentage changes
# Exclude the first year (where perc_diff is NA)
dt_region_filtered <- dt_region[Year != min(Year)]
# Then you can plot
ggplot(dt_region_filtered, aes(x = Year, y = perc_diff, color = Region)) +
geom_line(size = 1.2) +
geom_point(size = 2) +
labs(title = "Year-to-Year Percentage Changes by Region",
y = "Percentage Change (%)",
x = "Year") +
theme_minimal()
res[[1]]
res[[2]]
res[[3]]
library(ggplot2)
# Step 1: Compute aggregated counts by Year and Region
dt_region <- dt[, .(agg_count = sum(Count)), by = .(Year, Region)]
dt_region <- calculate_percentage_differences(dt_region, count_var = "agg_count",
year_var = "Year", grouping_var = c("Region"))
# Step 3: Plot percentage changes
# Exclude the first year (where perc_diff is NA)
dt_region_filtered <- dt_region[Year != min(Year)]
# Then you can plot
ggplot(dt_region_filtered, aes(x = Year, y = perc_diff, color = Region)) +
geom_line(linewidth = 1.2) +
geom_point(size = 2) +
labs(title = "Year-to-Year Percentage Changes by Region",
y = "Percentage Change (%)",
x = "Year") +
theme_minimal()
library(data.table, quietly = T)
res[[1]]; res[[2]]; res[[3]]
