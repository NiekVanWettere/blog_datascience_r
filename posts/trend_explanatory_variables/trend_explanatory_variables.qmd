---
title: "Selection of explanatory variables for yearly trend"
author: "Niek Van Wettere"
date: "2026-01-15"
categories: [trend analysis]
format: 
  html:
    self-contained: true
    toc: true
    number-sections: true
    code-fold: false
    code-overflow: wrap
    df-print: paged
editor: source
---

<br />

# Introduction

This blog post demonstrates a method to progressively select explanatory variables that present different trends from the overall yearly trend.

<br />

# Load packages

We use the data.table package, which allows for fast computation.

```{r, message = F}

library(data.table, quietly = T)

```

<br />

# Overview of functions

Next, we create a series of R functions that will allow us to perform the variable analysis.

<br />

## Function to calculate percentage differences between consecutive years

```{r}

# Function to calculate percentage differences between consecutive years
calculate_percentage_differences <- function(dt, count_var, year_var, grouping_var) {
  
  # Ensure data is ordered by year
  dt <- dt[order(get(year_var))]
  
  # If grouping_var is empty, don't group, just calculate the percentage difference for the entire dataset
  if (length(grouping_var) > 0) {
    dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = "lag")) / shift(.SD[[count_var]], type = "lag")) * 100, by = grouping_var]
  } else {
    dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = "lag")) / shift(.SD[[count_var]], type = "lag")) * 100]
  }
  
  return(dt)
}

```

<br />

## Function to calculate deviation

```{r}

# Helper function to calculate deviation
calculate_deviation <- function(actual, expected, metric) {
  if (metric == "mad") return(mean(abs(actual - expected), na.rm = TRUE))
  if (metric == "mse") return(mean((actual - expected)^2, na.rm = TRUE))
  if (metric == "rmse") return(sqrt(mean((actual - expected)^2, na.rm = TRUE)))
  if (is.function(metric)) return(metric(actual, expected)) # Allow user-defined metrics
  stop("Unsupported deviation metric. Use 'mad', 'mse', 'rmse', or a custom function.")
}

```

<br />

## Function to select variables with highest deviation from main trend

```{r}

forward_selection_trends <- function(dt, included_vars, optional_vars, count_var = "Count", 
                                     year_var = "Year", agg_func = sum, min_row_threshold = 10, 
                                     stopping_threshold = 0.01, deviation_metric = "mad") {
  # Ensure the data is a data.table
  dt <- as.data.table(dt)
  included_vars <- as.character(included_vars); selected_vars <- included_vars
  optional_vars <- as.character(optional_vars); remaining_vars <- optional_vars
  
  # Check for missing data
  if (anyNA(dt)) {
    stop("The dataset contains missing values. Please clean or impute missing data before proceeding.")
  }
  
  selected_variables_and_deviations_at_each_step <- list() # To store selected variable and deviation at each step
  iteration_details <- list() # To store details of all variables in each iteration
  previous_deviation <- NA # Track deviation improvement
  
  start_time <- Sys.time()
  cat("Starting forward selection with minimum row threshold =", min_row_threshold, "...\n")
  
  # Iterate until no remaining variable improves variability
  for (step in seq_along(optional_vars)) {
    
    # Check if there are any remaining vars left. If not, exit.
    if (length(remaining_vars) == 0) break
    
    # Calculate the reference percentage differences to be compared with
    dt_ref_aggreg <- dt[, .(agg_count = agg_func(.SD[[count_var]])), by = c(year_var, selected_vars)]
    ref_perc_diffs <- calculate_percentage_differences(copy(dt_ref_aggreg), count_var = 'agg_count', year_var, grouping_var = c(selected_vars))
    
    # Track deviations for all remaining variables
    results <- lapply(remaining_vars, function(var) {
      
      # Aggregate the count variable using the custom function
      grouped <- dt[, agg_count := agg_func(.SD[[count_var]]), by = c(selected_vars, var, year_var)]
      
      # Ensure enough rows exist in each group
      if (min(grouped[, .N, by = c(selected_vars, var)]$N) < min_row_threshold) {
        return(NA) # Skip variables that would result in too few rows in groups
      }
      
      # Calculate percentage differences for each group
      grouped_perc_diffs <- calculate_percentage_differences(copy(grouped), count_var = 'agg_count', year_var, grouping_var = c(selected_vars, var))
      
      
      # Join with overall percentage differences to compare
      comparison <- merge(grouped_perc_diffs, ref_perc_diffs, 
                          by = c(selected_vars, year_var), all.x = TRUE)
      
      
      # Compute the deviation using the specified metric
      calculate_deviation(comparison$perc_diff.x, comparison$perc_diff.y, deviation_metric)
    })
    
    # Skip step if all remaining variables fail the row count threshold
    if (all(is.na(results))) {
      cat("Stopping: No variable meets the minimum row threshold.\n")
      break
    }
    
    # Capture deviations for all variables in the current iteration
    iteration_deviation <- data.table(
      Step = step,
      Variable = remaining_vars,
      Deviation = unlist(results)
    )
    iteration_details[[step]] <- iteration_deviation
    
    # Find the variable with the maximum deviation
    deviations_step <- unlist(results)
    max_deviation <- max(deviations_step, na.rm = TRUE)
    max_var <- remaining_vars[which.max(deviations_step)]
    
    # Stop if the deviation improvement is below the threshold
    if (!is.na(previous_deviation) && (max_deviation - previous_deviation) / abs(previous_deviation) < stopping_threshold) {
      cat("Stopping: Deviation improvement below threshold.\n")
      break
    }
    
    previous_deviation <- max_deviation
    
    # Record the result
    selected_variables_and_deviations_at_each_step[[step]] <- data.table(Step = step, Variable_with_max_deviation = max_var, Max_deviation = max_deviation)
    
    # Add the selected variable to included and remove from remaining
    selected_vars <- c(selected_vars, max_var)
    remaining_vars <- setdiff(remaining_vars, max_var)
    
    cat("Step", step, "- Variable selected:", max_var, "Deviation:", max_deviation, "\n")
  }
  
  end_time <- Sys.time()
  
  # Final summary report
  report <- list(
    total_iterations = length(selected_variables_and_deviations_at_each_step),
    selected_vars = selected_vars,
    excluded_vars = setdiff(optional_vars, selected_vars),
    execution_time = end_time - start_time,
    deviation_metric = deviation_metric
  )
  
  return(list(
    selected_variables_and_deviations_at_each_step = rbindlist(selected_variables_and_deviations_at_each_step), # Final selection of variables with their deviations
    iteration_details = rbindlist(iteration_details), # Details of all iterations
    report = report # Summary report
  ))
}

```

<br />

# Application

Finally, we apply the function to test data.

## Test data

The following code generates synthetic test data.

```{r}

set.seed(42)

# Define dimensions
years <- 2000:2010
regions <- c("North", "South", "East", "West")
products <- c("A", "B")
channels <- c("Online", "Retail")

# Create base table with all combinations
dt <- CJ(
  Year = years,
  Region = regions,
  Product = products,
  Channel = channels
)

# Base count
dt[, base := 100]

# Region-specific trends (strong signal)
region_trend <- c(
  North = 0.06,
  South = 0.03,
  East  = 0.00,
  West  = -0.02
)

# Product-specific trends (moderate signal)
product_trend <- c(
  A = 0.02,
  B = 0.00
)

# Channel-specific noise (small)
channel_noise <- c(
  Online = 0.005,
  Retail = -0.005
)

# Generate counts: exponential growth + small noise
dt[, Count :=
     base *
     exp(
       region_trend[Region] * (Year - min(Year)) +
         product_trend[Product] * (Year - min(Year)) +
         channel_noise[Channel] * (Year - min(Year))
     ) +
     rnorm(.N, mean = 0, sd = 2)  # small random noise
]

# Ensure positive integer counts
dt[, Count := pmax(round(Count), 1)]

# Remove helper column
dt[, base := NULL]

# Check a few rows
head(dt)

```

<br />

## Trend analysis

```{r}

# output res

res <- forward_selection_trends(
  dt,
  included_vars = character(0),
  optional_vars = c("Region", "Product", "Channel"),
  count_var = "Count",
  year_var = "Year",
  min_row_threshold = 10,
  deviation_metric = "mad"
)

```

<br />

```{r}

res[[1]]; res[[2]]; res[[3]]

```


<br />

## Visualization

```{r}

library(ggplot2)
# Step 1: Compute aggregated counts by Year and Region
dt_region <- dt[, .(agg_count = sum(Count)), by = .(Year, Region)]

dt_region <- calculate_percentage_differences(dt_region, count_var = "agg_count", 
                                              year_var = "Year", grouping_var = c("Region"))

# Step 3: Plot percentage changes
# Exclude the first year (where perc_diff is NA)
dt_region_filtered <- dt_region[Year != min(Year)]

# Then you can plot
ggplot(dt_region_filtered, aes(x = Year, y = perc_diff, color = Region)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 2) +
  labs(title = "Year-to-Year Percentage Changes by Region",
       y = "Percentage Change (%)",
       x = "Year") +
  theme_minimal()

```


<br />


