[
  {
    "objectID": "posts/period_aggregation/period_aggregation.html",
    "href": "posts/period_aggregation/period_aggregation.html",
    "title": "Aggregating overlapping time periods",
    "section": "",
    "text": "This blog post shows a method to aggregate overlapping time periods in R. First, we generate dummy data that is structured as follows: for each event, several periods are defined with a start and end date. Some of these time periods overlap, while there are also gaps between periods that are not covered by the different periods for each event.\n\nlibrary('tidyverse', quietly = T)\n\ndf_with_periods &lt;- tibble::tibble(\n  event = c(\n    rep(\"event1\", 4),\n    rep(\"event2\", 4),\n    rep(\"event3\", 4)\n  ),\n  start_date = as.Date(c(\n    \"2025-01-01\", \"2025-01-04\", \"2025-01-12\", \"2025-01-18\",   # event1\n    \"2025-01-03\", \"2025-01-05\", \"2025-01-14\", \"2025-01-20\",   # event2\n    \"2025-01-02\", \"2025-01-07\", \"2025-01-15\", \"2025-01-22\"    # event3\n  )),\n  end_date = as.Date(c(\n    \"2025-01-06\", \"2025-01-05\", \"2025-01-16\", \"2025-01-22\",   # event1\n    \"2025-01-07\", \"2025-01-12\", \"2025-01-18\", \"2025-01-25\",   # event2\n    \"2025-01-05\", \"2025-01-11\", \"2025-01-20\", \"2025-01-27\"    # event3\n  ))\n)\n\ndf_with_periods\n\n\n  \n\n\n\n\nThe goal is now to merge overlapping periods into one longer period, while preserving the gaps between periods. To tackle this issue, we create several R functions.\nFirst, we define a function that, per event, orders the rows according to begin date and takes a rolling maximum of the end date. The start date of the next row is also added.\n\ncalculate_rolling_max_end_date &lt;- function(data) {  # Order rows according to begin date and take rolling maximum of end date.\n  data |&gt;\n    select(event, start_date, end_date) |&gt;\n    distinct() |&gt;\n    group_by(event) |&gt;\n    arrange(start_date) |&gt;\n    mutate(\n      end_date = as.Date(cummax(as.numeric(end_date)), origin = \"1970-01-01\"),\n      next_start_date = lead(start_date)) |&gt; \n    ungroup()\n}\n\ndf_with_periods_aggregated &lt;- df_with_periods |&gt; calculate_rolling_max_end_date() |&gt; arrange(event)\ndf_with_periods_aggregated\n\n\n  \n\n\n\n\nThen, we search for the rows where there is a time gap between the end date and the ‘next start date’. This allows us to establish the time gaps, together with the oldest start date and the most recent end date. Since there are no complications of overlaps during the time gaps, it’s easier to first detect these gaps and reason from there to get the ‘unified’ time periods.\n\nidentify_gaps &lt;- function(data) {\n  gaps &lt;- data |&gt;   \n    filter(difftime(next_start_date, end_date, units = \"days\") &gt; 1) |&gt;  # Find the rows with gaps.\n    select(- start_date) |&gt;\n    rename(start_date = next_start_date) |&gt;\n    select(event, start_date, end_date)\n  \n  min_begin &lt;- data |&gt;\n    group_by(event) |&gt;\n    summarise(start_date = min(start_date), .groups = 'drop') |&gt;\n    mutate(end_date = NA)\n  \n  max_end &lt;- data |&gt;\n    group_by(event) |&gt;\n    summarise(end_date = max(end_date), .groups = 'drop') |&gt;\n    mutate(start_date = NA)\n  \n  gaps &lt;- bind_rows(gaps, min_begin, max_end) \n  \n  return(gaps)\n}\n\ndf_with_periods_aggregated_2 &lt;- df_with_periods_aggregated |&gt; identify_gaps() |&gt; arrange(event, start_date) \ndf_with_periods_aggregated_2 |&gt; select(event, end_date, start_date)\n\n\n  \n\n\n\n\nFinally, the aggregated intervals are calculated, based on the time gaps.\n\ncalculate_aggregated_intervals &lt;- function(data) {\n  data |&gt;\n    group_by(event) |&gt;\n    arrange(start_date) |&gt;\n    mutate(end_date = lead(end_date)) |&gt;\n    ungroup() |&gt;\n    filter(! is.na(start_date))\n}\n\ndf_with_periods_aggregated_3 &lt;- df_with_periods_aggregated_2 |&gt; calculate_aggregated_intervals() |&gt; arrange(event)\ndf_with_periods_aggregated_3"
  },
  {
    "objectID": "posts/histogram/index.html",
    "href": "posts/histogram/index.html",
    "title": "Histogram with median, interquartile range and outlier highlighting",
    "section": "",
    "text": "In this blog post, we explore how a histogram plot can be enriched with information about median, interquartile range and outlier highlighting.\nFirst, we load the tidyverse package.\n\nlibrary('tidyverse', quietly = T)\n\n\nDummy data is generated: test scores for four different ‘education levels’.\n\nset.seed(123)\n\nn &lt;- 1000\nn_outliers &lt;- 20  # number of outliers\n\n# Generate test scores mostly around 50 with sd 15\ntest_scores &lt;- rnorm(n, mean = 50, sd = 15)\n\n# Clip values between 0 and 100\ntest_scores &lt;- pmin(pmax(test_scores, 0), 100)\n\n# Add some extreme outliers\noutlier_indices &lt;- sample(1:n, n_outliers)\ntest_scores[outlier_indices] &lt;- sample(c(runif(n_outliers/2, 90, 100),  # high outliers\n                                         runif(n_outliers/2, 0, 10)))   # low outliers\n\n# Create education levels\neducation_levels &lt;- sample(c(\"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\"), n, replace = TRUE)\n\n# Combine into a dataframe\ndataframe_with_test_scores &lt;- data.frame(\n  test_scores = test_scores,\n  education_levels = education_levels\n)\n\n\nWe calculate median and IQR-limits to plot along the histogram.\n\nsummary_data_IQR &lt;- dataframe_with_test_scores |&gt;\n\n  group_by(education_levels) |&gt;\n\n  summarise(\n\n    number_of_students = n(),\n    median_test_score = median(test_scores),\n    IQR_lower = quantile(test_scores, 0.25),\n    IQR_upper = quantile(test_scores, 0.75), .groups = 'drop') |&gt;\n\n  mutate(IQR_range = (IQR_upper - IQR_lower) |&gt; round(2),\n\n         outlier_threshold_1 = IQR_upper + (IQR_range * 1.5),\n         outlier_threshold_2 = IQR_lower - (IQR_range * 1.5))\n\n\nThat information is joined with the original dataframe containing the test scores.\n\ntest_scores_per_education_level_list &lt;- dataframe_with_test_scores |&gt;\n  left_join(summary_data_IQR, by = join_by(education_levels))  |&gt;\n  group_split(education_levels) # Split in list of dataframes.\n\n\nWe create the histogram plots.\n\nThe median value is shown as a green vertical line.\nThe interquartile range is situated between the blue vertical lines.\nOutlier values are colored in red.\n\n\nlist_plots &lt;- vector(mode = \"list\", length = length(test_scores_per_education_level_list))\n\nfor (i in seq_along(test_scores_per_education_level_list))  { \n\n  # flexible bin-width\n  max_value &lt;- max(test_scores_per_education_level_list[[i]]$test_scores, na.rm = TRUE)\n  binwidth &lt;- max_value / 40 \n\n  # median IQR-range\n  median_coord &lt;- test_scores_per_education_level_list[[i]]$median_test_score[1]\n  median_label &lt;- paste0(test_scores_per_education_level_list[[i]]$median_test_score[1] |&gt; round(2), '%')\n  IQR_upper_x_coord &lt;- test_scores_per_education_level_list[[i]]$IQR_upper[1]\n  IQR_range_label &lt;- paste0('iqr: ', test_scores_per_education_level_list[[i]]$IQR_range[1], '%')\n\n  # plot histogram\nplot_result &lt;- test_scores_per_education_level_list[[i]] |&gt;\n  mutate(color = ifelse(test_scores &gt; outlier_threshold_1 | test_scores &lt; outlier_threshold_2, \"darkred\", \"grey\")) |&gt;\n\n \n  ggplot(aes(x = test_scores, fill = color)) +\n\n  facet_grid(cols = vars(education_levels), scales = 'fixed') +\n\n  scale_x_continuous(breaks = scales::pretty_breaks(), labels = scales::percent_format(scale = 1), expand = expansion(mult = c(0, 0.05))) +\n\n  scale_y_continuous(breaks = scales::pretty_breaks(), labels = scales::number_format(accuracy = 1)) +\n\n  geom_histogram(binwidth = binwidth) +\n\n  scale_fill_identity() +\n\ngeom_segment(aes(x = median_test_score, xend = median_test_score, y = 0, yend = Inf), color = \"darkgreen\", linewidth = 0.5, linetype = \"dashed\") +\n\n  geom_segment(aes(x = IQR_lower, xend = IQR_lower, y = 0, yend = Inf), color = \"blue\", linewidth = 0.5, linetype = \"dashed\") +\n\n  geom_segment(aes(x = IQR_upper, xend = IQR_upper, y = 0, yend = Inf), color = \"blue\", linewidth = 0.5, linetype = \"dashed\")  +\n\n  labs(x = \"test scores\", y = \"number of students\") +\n\n  theme_bw() +\n\n  theme(axis.title.x = element_text(size = 10),  # Adjust x-axis title size\n\n        axis.title.y = element_text(size = 10))   # Adjust y-axis title size\n\n\nmax_count_bin &lt;- max(ggplot_build(plot_result)$data[[1]]$count)\n\n# add text annotation for median and iqr-range\nplot_result_with_annotation &lt;- plot_result +\n\n  annotate(\"text\", x = IQR_upper_x_coord + (0.04 * max_value), y = 0.9 * max_count_bin, label = IQR_range_label, vjust = 1, hjust = 0, color = \"blue\", size = 3) +\n\nannotate(\"text\", x = median_coord, y = 0, label = median_label, vjust = 1.5, color = \"darkgreen\", size = 3) + expand_limits(y = - (max_count_bin * 0.1))\n\n\nlist_plots[[i]] &lt;- plot_result_with_annotation\n\n}\n\n\nNow we visualize one plot from the list of plots that was created.\n\nlist_plots[[1]]\n\n\n\n\n\n\n\n\n\nThe different graphs can also be combined in one panel.\n\nlayout_matrix &lt;- rbind(c(1, 2),\n\n                       c(3, 4))\n\ngridExtra::grid.arrange(grobs = list_plots, layout_matrix = layout_matrix, ncol = 2, top = \"Distribution of test scores per education level\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Niek Van Wettere did a PhD in corpus linguistics at Ghent University (Belgium) before transitioning into data jobs outside academia. This blog showcases a series of data topics and how they can be tackled with R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog about data science in R",
    "section": "",
    "text": "Automated evaluation of companies in ESG-screened ETF\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nDec 27, 2025\n\n\nNiek Van Wettere\n\n\n\n\n\n\n\n\n\n\n\n\nJob choice simulation with r-shinylive\n\n\n\napp\n\n\n\n\n\n\n\n\n\nDec 14, 2025\n\n\nNiek Van Wettere\n\n\n\n\n\n\n\n\n\n\n\n\nAggregating overlapping time periods\n\n\n\ndata wrangling\n\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\nNiek Van Wettere\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram with median, interquartile range and outlier highlighting\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nNov 2, 2025\n\n\nNiek Van Wettere\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/model_choices_in_new_job_situation/2025_Model_choices_in_new_job_situation.html",
    "href": "posts/model_choices_in_new_job_situation/2025_Model_choices_in_new_job_situation.html",
    "title": "Job choice simulation with r-shinylive",
    "section": "",
    "text": "Introduction\nThis blog post explores the use of r-shinylive to deploy an app about job choice simulation. Thanks to r-shinylive, the app can run autonomously in the browser of the end user, without a server.\nIn this app, we simulate job choice for four groups of employees, each with a different group size. There are four choice possibilities to accommodate for the new situation:\n\n‘remain in old job position’: we assume that a certain number of people have to remain in their old job position. All these positions must be filled and there cannot remain more people in their old job position than available positions.\n‘change to new job position in old field’: we assume that only a limited new job positions in the old field are available. Not all positions have to be filled by the existing workforce.\n‘change to new job position in new field’: we assume that there is no limitation for these positions.\n‘quit job’: the employee leaves the organization.\n\nFor each group of employees, probabilities can be associated with the different choices. These probabilities have to sum up to 1.\nThe app generates a table that estimates how many people will choose a certain outcome, based on multiple runs of the algorithm (cf. bootstrap reps). The algorithm encompasses different steps beyond the initial sampling of choice outcome: (1) ensure that all positions in the old job are covered and (2) ensure that, if a certain option is taken by too many people, these people are re-assigned to a new choice. These ‘second’ choices are reported in the diagnostics section.\n\n\nApp interface\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 750\n#| freeze: false\n\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# ============================================================\n# Helper functions\n# ============================================================\n\n# Function to assign people to other group, in order to fill must-filled positions--------------------------------------------\nassign_to_other_group &lt;- function(df,\n                                  target_choice,\n                                  required_n) {\n\n  # How many already assigned?\n  current_n &lt;- sum(df$choice == target_choice)\n  need &lt;- required_n - current_n\n\n  # If there are no extra people needed to fill positions, there is nothing to do.\n  if (need &lt;= 0) {\n    return(list(\n      df = df,\n      moved = 0,\n      shortage = 0\n    ))\n  }\n\n  # Pool of candidates NOT currently in the target group\n  pool &lt;- df |&gt; dplyr::filter(choice != target_choice)\n\n  collected &lt;- list()  # Empty list to collect people who were re-assigned.\n  iter &lt;- 1\n  max_iter &lt;- 100\n  \n  prob_cols &lt;- c(\n    \"choice_change_to_new_job_position_in_old_field\",\n    \"choice_change_to_new_job_position_in_new_field\",\n    \"choice_remain_in_old_job_position\",\n    \"choice_quit_job\"\n  )\n  \n  # WHILE LOOP: iterative probability-based sampling\n  while (\n    iter &lt;= max_iter &&  # We limit the while-loop to a maximum number of iterations.\n    length(dplyr::bind_rows(collected)$person_id %||% character()) &lt; need &&  # If the number of needed people is attained, the while-loop can stop.\n    nrow(pool) &gt; 0   # If the pool is empty, the while-loop is stopped.\n  ) {\n\n    iter &lt;- iter + 1\n\n    # Reassign choices using probabilities\n    pool &lt;- pool |&gt;\n      rowwise() |&gt;\n      dplyr::mutate(choice = sample(prob_cols, 1, replace = F, prob=c_across(all_of(prob_cols)))) |&gt; \n      ungroup()\n\n    # Individuals reassigned to target_choice in this iteration\n    moved_this_round &lt;- pool |&gt; dplyr::filter(choice == target_choice)\n\n    if (nrow(moved_this_round) &gt; 0)\n      collected[[length(collected) + 1]] &lt;- moved_this_round  # Reassigned people are added to the collected-list.\n\n    # Remaining pool loses those moved this round.\n    pool &lt;- pool |&gt; dplyr::filter(choice != target_choice)\n  }\n\n  # Bind all collected people\n  moved_people &lt;- dplyr::bind_rows(collected)\n\n  moved_n &lt;- nrow(moved_people)\n  shortage &lt;- max(0, need - moved_n)\n\n  # Build final dataset\n  output &lt;- df |&gt;\n    # keep original target_choice people\n    dplyr::filter(choice == target_choice) |&gt;\n    # add people who stayed with non-target choices\n    dplyr::union_all(pool) |&gt;\n    # add collected reassigned people\n    dplyr::union_all(moved_people) |&gt;\n    dplyr::ungroup()\n\n  list(\n    df = output,\n    moved = moved_n,\n    shortage = shortage\n  )\n}\n\n\n# If there are too many people for a given choice (lack of positions), the group is 'downsized' -------------------------------- \nlimit_group &lt;- function(df, choice_name, threshold) {\n  df_in  &lt;- df |&gt; filter(choice == choice_name)\n  df_out &lt;- df |&gt; filter(choice != choice_name)\n  \n  reassigned_n &lt;- 0\n  \n  if (nrow(df_in) &gt; threshold) {\n    keep &lt;- df_in |&gt; slice_sample(n=threshold)  # We randomly sample the people who will stick to their first choice.\n    excess &lt;- anti_join(df_in, keep, by=\"person_id\")  # The 'excess'-part will sample a new choice.\n    reassigned_n &lt;- nrow(excess)\n    \n    prob_cols &lt;- grep(\"^choice_\", names(df), value = TRUE)\n    excess[, choice_name] &lt;- 0  # The current choice gets zero probability and can't be chosen anymore.\n    \n    excess &lt;- excess |&gt;\n      rowwise() |&gt;\n      mutate(across(all_of(prob_cols), ~ .x / sum(c_across(all_of(prob_cols))))) |&gt;  # Ensure that probabilities sum to 1 again.\n      mutate(choice = sample(prob_cols, 1, replace = F, prob = c_across(all_of(prob_cols)))) |&gt;  # Sample new choice.\n      ungroup()\n    \n    df &lt;- bind_rows(df_out, keep, excess)\n  }\n  \n  list(df=df, reassigned=reassigned_n)\n}\n\n# Run single simulation -----------------------------------------------------------------------------------------------------------\nrun_single_sim &lt;- function(df, new_positions, old_positions) {\n  prob_cols &lt;- c(\n    \"choice_change_to_new_job_position_in_old_field\",\n    \"choice_change_to_new_job_position_in_new_field\",\n    \"choice_remain_in_old_job_position\",\n    \"choice_quit_job\"\n  )\n  \n  expanded &lt;- df |&gt;\n    uncount(weights = group_size) |&gt;\n    mutate(person_id = row_number()) |&gt;\n    rowwise() |&gt;\n    mutate(choice = sample(prob_cols, 1, prob=c_across(all_of(prob_cols)))) |&gt;  # Sample initial choice.\n    ungroup()\n  \n  # Following steps to adjust initial choices, in accordance with the available positions.\n  step1 &lt;- assign_to_other_group(expanded, \"choice_remain_in_old_job_position\", old_positions)\n  expanded &lt;- step1$df\n  \n  step2 &lt;- limit_group(expanded, \"choice_remain_in_old_job_position\", old_positions)\n  expanded &lt;- step2$df\n  \n  step3 &lt;- limit_group(expanded, \"choice_change_to_new_job_position_in_old_field\", new_positions)\n  expanded &lt;- step3$df\n  \n  list(\n    summary = expanded |&gt;\n      group_by(choice) |&gt;\n      summarize(n=n(), .groups=\"drop\"),\n    step1=step1,\n    step2=step2,\n    step3=step3\n  )\n}\n\n\n# Run bootstrap -----------------------------------------------------------------------------------------------------------\nrun_bootstrap &lt;- function(df, new_positions, old_positions, B, progress) {\n  \n  results &lt;- vector(\"list\", B)\n  messages &lt;- list(step1=0, step2=0, step3=0, shortage=0)\n  \n  for (i in 1:B) {\n    progress$set(value=i)\n    out &lt;- run_single_sim(df, new_positions, old_positions)   # We run the simulation.\n    \n    results[[i]] &lt;- out$summary\n    messages$step1    &lt;- messages$step1 + out$step1$moved\n    messages$shortage &lt;- messages$shortage + out$step1$shortage\n    messages$step2    &lt;- messages$step2 + out$step2$reassigned\n    messages$step3    &lt;- messages$step3 + out$step3$reassigned\n  }\n  \n  agg &lt;- bind_rows(results, .id=\"run\") |&gt;\n    group_by(choice) |&gt;\n    summarize(min=min(n), mean=round(mean(n),1), max=max(n), .groups=\"drop\")  # Summary statistics are established over multiple runs.\n  \n  list(summary=agg, messages=messages)\n}\n\n\n\n# ============================================================\n# UI — spreadsheet built from numericInputs (ShinyLive compatible)\n# ============================================================\n\nmake_prob_row &lt;- function(id_prefix, row_label, values) {\n  fluidRow(\n    column(2, strong(row_label)),\n    column(2, numericInput(paste0(id_prefix, \"_old\"),  \"Old field\",    values[1], min=0, max=1, step=0.05)),  # prob between 0 and 1\n    column(2, numericInput(paste0(id_prefix, \"_new\"),  \"New field\",    values[2], min=0, max=1, step=0.05)),\n    column(2, numericInput(paste0(id_prefix, \"_remain\"),\"Remain\",      values[3], min=0, max=1, step=0.05)),\n    column(2, numericInput(paste0(id_prefix, \"_quit\"), \"Quit\",         values[4], min=0, max=1, step=0.05)),\n    column(2, numericInput(paste0(id_prefix, \"_size\"), \"Size\",         values[5], min=0, step=1))  # size without max\n  )\n}\n\n\nui &lt;- fluidPage(\n  titlePanel(\"Job Choice Simulation\"),\n  \n  fluidRow(\n    \n    # Smaller left sidebar\n    column(\n      width = 3,\n      wellPanel(\n        numericInput(\"new_positions\", \"New job positions in old field:\", 200),\n        numericInput(\"old_positions\", \"Remaining old job positions:\", 100),\n        numericInput(\"B\", \"Bootstrap reps:\", 50),  # The number of bootstrap runs can be adjusted.\n        br(),\n        actionButton(\"run\", \"Run Simulation\")\n      )\n    ),\n    \n    # Wide main area\n    column(\n      width = 9,\n      \n      h4(\"Edit group probabilities:\"),   # Input probabilities.\n      make_prob_row(\"g1\", \"Group 1\", c(0.5, 0.2, 0.1, 0.2, 100)),\n      make_prob_row(\"g2\", \"Group 2\", c(0.2, 0.2, 0.4, 0.2, 200)),\n      make_prob_row(\"g3\", \"Group 3\", c(0.1, 0.2, 0.2, 0.5, 25)),\n      make_prob_row(\"g4\", \"Group 4\", c(0.4, 0.4, 0.2, 0.0, 75)),\n      \n      htmlOutput(\"warning_msg\"),\n      \n      hr(),\n      h3(\"Simulation summary\"),\n      tableOutput(\"summary\"),    # Summary output.\n      \n      hr(),\n      h3(\"Diagnostics\"),\n      verbatimTextOutput(\"diag\")   # Diagnostics output.\n    )\n  )\n)\n\n\n\n# ============================================================\n# SERVER\n# ============================================================\n\nserver &lt;- function(input, output, session) {\n  \n  # Collect table from numericInputs\n  collect_df &lt;- reactive({\n    data.frame(\n      group = paste(\"Group\", 1:4),\n      old_field_prob = c(input$g1_old, input$g2_old, input$g3_old, input$g4_old),\n      new_field_prob = c(input$g1_new, input$g2_new, input$g3_new, input$g4_new),\n      remain_prob    = c(input$g1_remain, input$g2_remain, input$g3_remain, input$g4_remain),\n      quit_prob      = c(input$g1_quit, input$g2_quit, input$g3_quit, input$g4_quit),\n      size           = c(input$g1_size, input$g2_size, input$g3_size, input$g4_size)\n    )\n  })\n  \n  # Row-sum validation warning\n  output$warning_msg &lt;- renderUI({\n    df &lt;- collect_df()\n    row_sums &lt;- df$old_field_prob + df$new_field_prob + df$remain_prob + df$quit_prob\n    if (any(abs(row_sums - 1) &gt; 1e-6)) {\n      HTML(\"&lt;span style='color:red;font-weight:bold;'&gt;Probabilities in every row must sum to 1.&lt;/span&gt;\")\n    } else \"\"\n  })\n  \n  sim_res &lt;- reactiveVal(NULL)\n  diag_res &lt;- reactiveVal(NULL)\n  \n  observeEvent(input$run, {   # cf. action button 'run simulation'\n    df &lt;- collect_df()\n    row_sums &lt;- df$old_field_prob + df$new_field_prob + df$remain_prob + df$quit_prob\n    if (any(abs(row_sums - 1) &gt; 1e-6)) {\n      showNotification(\"Fix probability rows before running!\", type=\"error\")   # Again check that probabilites sum to 1 upon run.\n      return()\n    }\n    \n    sim_df &lt;- data.frame(\n      choice_change_to_new_job_position_in_old_field = df$old_field_prob,\n      choice_change_to_new_job_position_in_new_field = df$new_field_prob,\n      choice_remain_in_old_job_position              = df$remain_prob,\n      choice_quit_job                                = df$quit_prob,\n      group_size                                     = df$size\n    )\n    \n    progress &lt;- Progress$new(session, min=0, max=input$B)   # Progess bar that monitors the bootstrap runs.\n    progress$set(message=\"Running simulation...\")\n    on.exit(progress$close(), add=TRUE)\n    \n    out &lt;- run_bootstrap(sim_df, input$new_positions, input$old_positions, input$B, progress)\n    \n    sim_res(out$summary)\n    diag_res(out$messages)\n  })\n  \n  output$summary &lt;- renderTable(sim_res())   # Output summary.\n  \n  output$diag &lt;- renderText({     # Output diagnostics.\n    d &lt;- diag_res()\n    if (is.null(d)) return(\"No simulation yet.\")\n    \n    paste0(\n      \"Remain in old job, shortage after re-sampling: \", d$shortage, \"\\n\",\n      \"REmain in old job, reassigned to attain full coverage: \", d$step1, \"\\n\",\n      \"Reassigned due to remain limit: \", d$step2, \"\\n\",\n      \"Reassigned due to old-field limit: \", d$step3, \"\\n\"\n    )\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/ESG/ESG.html#create-virtual-environment",
    "href": "posts/ESG/ESG.html#create-virtual-environment",
    "title": "Automated evaluation of companies in ESG-screened ETF",
    "section": "1.1 Create virtual environment",
    "text": "1.1 Create virtual environment\nFirst, we have to set up a virtual environment with the necessary packages.\n\nreticulate::virtualenv_create(\n  envname = \"esg-py\",\n  packages = c(\"pandas\", \"requests\", \"lxml\")\n)\n\n\nAdditionally, we create a requirements.txt file.\n\n# requirements.txt\npython &lt;- reticulate::py_config()$python\nsystem(sprintf('\"%s\" -m pip freeze &gt; requirements.txt', python))\n\n\nExtra packages can be installed into the virtual environment via the command below.\n\n# install extra packages\nreticulate::py_install(\"matplotlib\", envname = \"esg-py\")"
  },
  {
    "objectID": "posts/ESG/ESG.html#activate-virtual-environment",
    "href": "posts/ESG/ESG.html#activate-virtual-environment",
    "title": "Automated evaluation of companies in ESG-screened ETF",
    "section": "1.2 Activate virtual environment",
    "text": "1.2 Activate virtual environment\nOnce the virtual environment is created, we have to activate it so that the installed packages are available for further analysis of the data.\n\nlibrary(reticulate)\nreticulate::use_virtualenv(\"esg-py\", required = TRUE)"
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-calculate-percentage-differences-between-consecutive-years",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-calculate-percentage-differences-between-consecutive-years",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "3.1 Function to calculate percentage differences between consecutive years",
    "text": "3.1 Function to calculate percentage differences between consecutive years\n\n# Function to calculate percentage differences between consecutive years\ncalculate_percentage_differences &lt;- function(dt, count_var, year_var, grouping_var) {\n  \n  # Ensure data is ordered by year\n  dt &lt;- dt[order(get(year_var))]\n  \n  # If grouping_var is empty, don't group, just calculate the percentage difference for the entire dataset\n  if (length(grouping_var) &gt; 0) {\n    dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = \"lag\")) / shift(.SD[[count_var]], type = \"lag\")) * 100, by = grouping_var]\n  } else {\n    dt[, perc_diff := ((.SD[[count_var]] - shift(.SD[[count_var]], type = \"lag\")) / shift(.SD[[count_var]], type = \"lag\")) * 100]\n  }\n  \n  return(dt)\n}"
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-calculate-deviation",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-calculate-deviation",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "3.2 Function to calculate deviation",
    "text": "3.2 Function to calculate deviation\n\n# Helper function to calculate deviation\ncalculate_deviation &lt;- function(actual, expected, metric) {\n  if (metric == \"mad\") return(mean(abs(actual - expected), na.rm = TRUE))\n  if (metric == \"mse\") return(mean((actual - expected)^2, na.rm = TRUE))\n  if (metric == \"rmse\") return(sqrt(mean((actual - expected)^2, na.rm = TRUE)))\n  if (is.function(metric)) return(metric(actual, expected)) # Allow user-defined metrics\n  stop(\"Unsupported deviation metric. Use 'mad', 'mse', 'rmse', or a custom function.\")\n}"
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-select-variables-with-highest-deviation-from-main-trend",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#function-to-select-variables-with-highest-deviation-from-main-trend",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "3.3 Function to select variables with highest deviation from main trend",
    "text": "3.3 Function to select variables with highest deviation from main trend\n\nforward_selection_trends &lt;- function(dt, included_vars, optional_vars, count_var = \"Count\", \n                                     year_var = \"Year\", agg_func = sum, min_row_threshold = 10, \n                                     stopping_threshold = 0.01, deviation_metric = \"mad\") {\n  # Ensure the data is a data.table\n  dt &lt;- as.data.table(dt)\n  included_vars &lt;- as.character(included_vars); selected_vars &lt;- included_vars\n  optional_vars &lt;- as.character(optional_vars); remaining_vars &lt;- optional_vars\n  \n  # Check for missing data\n  if (anyNA(dt)) {\n    stop(\"The dataset contains missing values. Please clean or impute missing data before proceeding.\")\n  }\n  \n  selected_variables_and_deviations_at_each_step &lt;- list() # To store selected variable and deviation at each step\n  iteration_details &lt;- list() # To store details of all variables in each iteration\n  previous_deviation &lt;- NA # Track deviation improvement\n  \n  start_time &lt;- Sys.time()\n  cat(\"Starting forward selection with minimum row threshold =\", min_row_threshold, \"...\\n\")\n  \n  # Iterate until no remaining variable improves variability\n  for (step in seq_along(optional_vars)) {\n    \n    # Check if there are any remaining vars left. If not, exit.\n    if (length(remaining_vars) == 0) break\n    \n    # Calculate the reference percentage differences to be compared with\n    dt_ref_aggreg &lt;- dt[, .(agg_count = agg_func(.SD[[count_var]])), by = c(year_var, selected_vars)]\n    ref_perc_diffs &lt;- calculate_percentage_differences(copy(dt_ref_aggreg), count_var = 'agg_count', year_var, grouping_var = c(selected_vars))\n    \n    # Track deviations for all remaining variables\n    results &lt;- lapply(remaining_vars, function(var) {\n      \n      # Aggregate the count variable using the custom function\n      grouped &lt;- dt[, agg_count := agg_func(.SD[[count_var]]), by = c(selected_vars, var, year_var)]\n      \n      # Ensure enough rows exist in each group\n      if (min(grouped[, .N, by = c(selected_vars, var)]$N) &lt; min_row_threshold) {\n        return(NA) # Skip variables that would result in too few rows in groups\n      }\n      \n      # Calculate percentage differences for each group\n      grouped_perc_diffs &lt;- calculate_percentage_differences(copy(grouped), count_var = 'agg_count', year_var, grouping_var = c(selected_vars, var))\n      \n      \n      # Join with overall percentage differences to compare\n      comparison &lt;- merge(grouped_perc_diffs, ref_perc_diffs, \n                          by = c(selected_vars, year_var), all.x = TRUE)\n      \n      \n      # Compute the deviation using the specified metric\n      calculate_deviation(comparison$perc_diff.x, comparison$perc_diff.y, deviation_metric)\n    })\n    \n    # Skip step if all remaining variables fail the row count threshold\n    if (all(is.na(results))) {\n      cat(\"Stopping: No variable meets the minimum row threshold.\\n\")\n      break\n    }\n    \n    # Capture deviations for all variables in the current iteration\n    iteration_deviation &lt;- data.table(\n      Step = step,\n      Variable = remaining_vars,\n      Deviation = unlist(results)\n    )\n    iteration_details[[step]] &lt;- iteration_deviation\n    \n    # Find the variable with the maximum deviation\n    deviations_step &lt;- unlist(results)\n    max_deviation &lt;- max(deviations_step, na.rm = TRUE)\n    max_var &lt;- remaining_vars[which.max(deviations_step)]\n    \n    # Stop if the deviation improvement is below the threshold\n    if (!is.na(previous_deviation) && (max_deviation - previous_deviation) / abs(previous_deviation) &lt; stopping_threshold) {\n      cat(\"Stopping: Deviation improvement below threshold.\\n\")\n      break\n    }\n    \n    previous_deviation &lt;- max_deviation\n    \n    # Record the result\n    selected_variables_and_deviations_at_each_step[[step]] &lt;- data.table(Step = step, Variable_with_max_deviation = max_var, Max_deviation = max_deviation)\n    \n    # Add the selected variable to included and remove from remaining\n    selected_vars &lt;- c(selected_vars, max_var)\n    remaining_vars &lt;- setdiff(remaining_vars, max_var)\n    \n    cat(\"Step\", step, \"- Variable selected:\", max_var, \"Deviation:\", max_deviation, \"\\n\")\n  }\n  \n  end_time &lt;- Sys.time()\n  \n  # Final summary report\n  report &lt;- list(\n    total_iterations = length(selected_variables_and_deviations_at_each_step),\n    selected_vars = selected_vars,\n    excluded_vars = setdiff(optional_vars, selected_vars),\n    execution_time = end_time - start_time,\n    deviation_metric = deviation_metric\n  )\n  \n  return(list(\n    selected_variables_and_deviations_at_each_step = rbindlist(selected_variables_and_deviations_at_each_step), # Final selection of variables with their deviations\n    iteration_details = rbindlist(iteration_details), # Details of all iterations\n    report = report # Summary report\n  ))\n}"
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#test-data",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#test-data",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "4.1 Test data",
    "text": "4.1 Test data\nThe following code generates synthetic test data.\n\nset.seed(42)\n\n# Define dimensions\nyears &lt;- 2000:2010\nregions &lt;- c(\"North\", \"South\", \"East\", \"West\")\nproducts &lt;- c(\"A\", \"B\")\nchannels &lt;- c(\"Online\", \"Retail\")\n\n# Create base table with all combinations\ndt &lt;- CJ(\n  Year = years,\n  Region = regions,\n  Product = products,\n  Channel = channels\n)\n\n# Base count\ndt[, base := 100]\n\n# Region-specific trends (strong signal)\nregion_trend &lt;- c(\n  North = 0.06,\n  South = 0.03,\n  East  = 0.00,\n  West  = -0.02\n)\n\n# Product-specific trends (moderate signal)\nproduct_trend &lt;- c(\n  A = 0.02,\n  B = 0.00\n)\n\n# Channel-specific noise (small)\nchannel_noise &lt;- c(\n  Online = 0.005,\n  Retail = -0.005\n)\n\n# Generate counts: exponential growth + small noise\ndt[, Count :=\n     base *\n     exp(\n       region_trend[Region] * (Year - min(Year)) +\n         product_trend[Product] * (Year - min(Year)) +\n         channel_noise[Channel] * (Year - min(Year))\n     ) +\n     rnorm(.N, mean = 0, sd = 2)  # small random noise\n]\n\n# Ensure positive integer counts\ndt[, Count := pmax(round(Count), 1)]\n\n# Remove helper column\ndt[, base := NULL]\n\n# Check a few rows\nhead(dt)"
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#trend-analysis",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#trend-analysis",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "4.2 Trend analysis",
    "text": "4.2 Trend analysis\n\n# output res\n\nres &lt;- forward_selection_trends(\n  dt,\n  included_vars = character(0),\n  optional_vars = c(\"Region\", \"Product\", \"Channel\"),\n  count_var = \"Count\",\n  year_var = \"Year\",\n  min_row_threshold = 10,\n  deviation_metric = \"mad\"\n)\n\nStarting forward selection with minimum row threshold = 10 ...\nStep 1 - Variable selected: Region Deviation: 3.146615 \nStopping: Deviation improvement below threshold.\n\n\n\n\nres[[1]]; res[[2]]; res[[3]]\n\n\n  \n\n\n\n\n  \n\n\n\n$total_iterations\n[1] 1\n\n$selected_vars\n[1] \"Region\"\n\n$excluded_vars\n[1] \"Product\" \"Channel\"\n\n$execution_time\nTime difference of 0.08608603 secs\n\n$deviation_metric\n[1] \"mad\""
  },
  {
    "objectID": "posts/trend_explanatory_variables/trend_explanatory_variables.html#visualization",
    "href": "posts/trend_explanatory_variables/trend_explanatory_variables.html#visualization",
    "title": "Selection of explanatory variables for yearly trend",
    "section": "4.3 Visualization",
    "text": "4.3 Visualization\n\nlibrary(ggplot2)\n# Step 1: Compute aggregated counts by Year and Region\ndt_region &lt;- dt[, .(agg_count = sum(Count)), by = .(Year, Region)]\n\ndt_region &lt;- calculate_percentage_differences(dt_region, count_var = \"agg_count\", \n                                              year_var = \"Year\", grouping_var = c(\"Region\"))\n\n# Step 3: Plot percentage changes\n# Exclude the first year (where perc_diff is NA)\ndt_region_filtered &lt;- dt_region[Year != min(Year)]\n\n# Then you can plot\nggplot(dt_region_filtered, aes(x = Year, y = perc_diff, color = Region)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Year-to-Year Percentage Changes by Region\",\n       y = \"Percentage Change (%)\",\n       x = \"Year\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/ESG/ESG.html",
    "href": "posts/ESG/ESG.html",
    "title": "Automated evaluation of companies in ESG-screened ETF",
    "section": "",
    "text": "This blog post is about automated evaluation of companies listed in an ESG-screened ETF. We focus on the ETF ‘Xtrackers MSCI World ESG UCITS’. For each company in the ETF, we try to gather news and classify the news according to its negative ESG-value.\nMost of the analysis is done in Python, because Python offers more readily available tools for this type of analysis."
  }
]